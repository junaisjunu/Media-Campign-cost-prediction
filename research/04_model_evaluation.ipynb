{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\junu\\\\OneDrive\\\\Desktop\\\\ds\\\\MLOPS\\\\media_campign_cost_prediction\\\\branches\\\\basic-set-up\\\\Media-Campign-cost-prediction\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "# os.chdir('Media-Campign-cost-prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path \n",
    "\n",
    "@dataclass \n",
    "class ModelEvaluationConfig:\n",
    "    model_evaluation_root: Path\n",
    "    model_path: Path\n",
    "    evaluation_score: Path\n",
    "    test_data_path: Path\n",
    "    target: str\n",
    "    params: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constants import *\n",
    "from src.utils.common import read_yaml,create_directories,save_model\n",
    "\n",
    "class Configuration:\n",
    "    def __init__(self,config_path=CONFIG_FILE_PATH\n",
    "                      ,params_path=PARAMS_FILE_PATH\n",
    "                      ,schema_path=SCHEMA_FILE_PATH) -> None:\n",
    "        self.config=read_yaml(config_path)\n",
    "        self.params=read_yaml(params_path)\n",
    "        self.schema=read_yaml(schema_path)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self):\n",
    "        config= self.config.model_evaluation\n",
    "        model_eval_config=ModelEvaluationConfig(\n",
    "            model_evaluation_root=Path(config.model_evaluation_root),\n",
    "            model_path=Path(config.model_path),\n",
    "            evaluation_score=Path(config.evaluation_score),\n",
    "            test_data_path=Path(config.test_data_path),\n",
    "            target= config.target,\n",
    "            params= dict(self.params.xgboost)\n",
    "\n",
    "        )\n",
    "        return model_eval_config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['MLFLOW_TRACKING_URI']='https://dagshub.com/junaisk456/Media-Campign-cost-prediction.mlflow'\n",
    "os.environ['MLFLOW_TRACKING_USERNAME']='junaisk456'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD']='a4dcf1ebabde463c427fa51ac0272a6b4b0341e6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.common import load_model,create_directories,save_json\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score,mean_squared_log_error\n",
    "from src import logger\n",
    "import mlflow\n",
    "from mlflow.models.signature import infer_signature\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self,config: ModelEvaluationConfig) -> None:\n",
    "        self.config=config\n",
    "        \n",
    "    def initiate_evaluation(self):\n",
    "        model=load_model(self.config.model_path)\n",
    "        eval_data=pd.read_csv(self.config.test_data_path)\n",
    "        X=eval_data.drop(self.config.target,axis=1)\n",
    "        y_test=eval_data[self.config.target]\n",
    "        logger.info(self.config.params)\n",
    "        params=self.config.params\n",
    "        #model prediction\n",
    "        y_pred=model.predict(X)\n",
    "        create_directories([self.config.model_evaluation_root])\n",
    "        mae=mean_absolute_error(y_test,y_pred)\n",
    "        rmse=mean_squared_error(y_test,y_pred,squared=False)\n",
    "        r2=r2_score(y_test,y_pred)\n",
    "        rmlse=mean_squared_log_error(y_test,y_pred,squared=False)\n",
    "        logger.info(f\" model - \\n MAE = {mae} \\n RMSE = {rmse} \\n R2 Score = {r2} \\n RMLSE = {rmlse} \\n ============================\")\n",
    "\n",
    "        metrics={\"MAE\":mae,\n",
    "               \"RMSE\":rmse,\n",
    "               \"r2\":r2,\n",
    "               \"rmlse\":rmlse}\n",
    "        \n",
    "        eval_score={\n",
    "            \"Model Parms\":params,\n",
    "            \"evaluation metrics\":metrics\n",
    "\n",
    "        }\n",
    "        save_json(self.config.evaluation_score,eval_score)\n",
    "        # Set our tracking server uri for logging\n",
    "        mlflow.set_tracking_uri(uri=\"https://dagshub.com/junaisk456/Media-Campign-cost-prediction.mlflow\")\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "        mlflow.set_experiment(\"Media Cost prediction Tracker\")\n",
    "\n",
    "# Start an MLflow run\n",
    "        with mlflow.start_run():\n",
    "    # Log the hyperparameters\n",
    "            mlflow.log_params(params)\n",
    "\n",
    "    # Log the loss metric\n",
    "            mlflow.log_metrics(metrics)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "            mlflow.set_tag(\"Training Info\", \"Xg boost model\")\n",
    "\n",
    "    # Infer the model signature\n",
    "            signature = infer_signature(model_input=X,model_output=y_pred)\n",
    "\n",
    "            # Log the model\n",
    "            model_info = mlflow.xgboost.log_model(\n",
    "                xgb_model=model,\n",
    "                artifact_path=\"artifacts\",\n",
    "                signature=signature,\n",
    "                input_example=X,\n",
    "                registered_model_name=\"XGBoost model1\",\n",
    "            )\n",
    "\n",
    "\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-04 23:11:08,907: INFO: 19 : call: file loaded succesfully from the path config\\config.yaml]\n",
      "[2024-04-04 23:11:08,928: INFO: 19 : call: file loaded succesfully from the path params.yaml]\n",
      "[2024-04-04 23:11:08,941: INFO: 19 : call: file loaded succesfully from the path schema.yaml]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-04-04 23:11:09,261: INFO: 17 : call: {'objective': 'reg:squarederror', 'booster': 'gbtree', 'learning_rate': 0.3, 'max_depth': 6, 'alpha': 8, 'n_estimators': 40, 'gamma': 3, 'verbosity': 2, 'min_child_weight': 2, 'reg_lambda': 1, 'scale_pos_weight': 4, 'eval_metric': 'rmse'}]\n",
      "[2024-04-04 23:11:09,387: INFO: 26 : call:  model - \n",
      " MAE = 24.25746600866434 \n",
      " RMSE = 28.374718588914945 \n",
      " R2 Score = 0.10173379007128436 \n",
      " RMLSE = 0.30340002914546205 \n",
      " ============================]\n",
      "[2024-04-04 23:11:09,390: INFO: 38 : call: json file save successfully at artifacts\\model_evaluation\\score.json]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/04/04 23:11:10 INFO mlflow.tracking.fluent: Experiment with name 'Media Cost prediction Tracker' does not exist. Creating a new experiment.\n",
      "c:\\Users\\junu\\anaconda3\\envs\\media\\lib\\site-packages\\mlflow\\models\\signature.py:130: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  inputs = _infer_schema(model_input)\n",
      "c:\\Users\\junu\\anaconda3\\envs\\media\\lib\\site-packages\\xgboost\\core.py:160: UserWarning: [23:11:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\junu\\anaconda3\\envs\\media\\lib\\site-packages\\_distutils_hack\\__init__.py:33: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "Successfully registered model 'XGBoost model1'.\n",
      "2024/04/04 23:11:37 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: XGBoost model1, version 1\n",
      "Created version '1' of model 'XGBoost model1'.\n"
     ]
    }
   ],
   "source": [
    "config=Configuration()\n",
    "eval_config=config.get_model_evaluation_config()\n",
    "eval=ModelEvaluation(eval_config)\n",
    "eval.initiate_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrcis={\n",
    "    \"name\":\"j u\",\n",
    "    \"age\":25\n",
    "}\n",
    "path=\n",
    "\n",
    "save_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!mlflow server --host 127.0.0.1 --port 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_TRACKING_URI=https://dagshub.com/junaisk456/Media-Campign-cost-prediction.mlflow \\\n",
    "MLFLOW_TRACKING_USERNAME=junaisk456 \\\n",
    "MLFLOW_TRACKING_PASSWORD=a4dcf1ebabde463c427fa51ac0272a6b4b0341e6 \\\n",
    "python script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMLFLOW_TRACKING_URI\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://dagshub.com/junaisk456/Media-Campign-cost-prediction.mlflow\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'os' has no attribute 'env'"
     ]
    }
   ],
   "source": [
    "os.env['MLFLOW_TRACKING_URI']='https://dagshub.com/junaisk456/Media-Campign-cost-prediction.mlflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "media",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
